{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/d.bhatt/Multitask-RFG/MTrfg_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## This is to get results of our models trained on 30 splits. We have results of all 30 splits\n",
    "## and this notebbook will consolidate them at one place \n",
    "## imports!\n",
    "from mtrfg.utils import load_json, get_overall_results\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## path to test and validation results for all the splits\n",
    "\n",
    "test_jsons = glob.glob('/user/d.bhatt/Multitask-RFG/saved_models/30_splits_with_new_softmax/*/test_results.json')\n",
    "val_jsons = glob.glob('/user/d.bhatt/Multitask-RFG/saved_models/30_splits_with_new_softmax/*/val_results_best.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading results from above paths!\n",
    "\n",
    "test_results = [load_json(test_file) for test_file in test_jsons]\n",
    "val_results = [load_json(val_file) for val_file in val_jsons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on test dataset for 30 runs:\n",
      "tagger_results: Precision: 0.8892 ± 0.0122, Max precision: 0.9086, Min precision: 0.8633\n",
      "                Recall: 0.8872 ± 0.014, Max Recall: 0.9148, Min Recall: 0.8345\n",
      "                F1: 0.8881 ± 0.0117, Max F1: 0.9069, Min F1: 0.8486\n",
      "\n",
      "parser_labeled_results: Precision: 0.6739 ± 0.0175, Max precision: 0.7097, Min precision: 0.6434\n",
      "                        Recall: 0.6476 ± 0.0193, Max Recall: 0.6927, Min Recall: 0.6112\n",
      "                        F1: 0.6604 ± 0.0177, Max F1: 0.7011, Min F1: 0.6329\n",
      "\n",
      "parser_unlabeled_results: Precision: 0.7518 ± 0.0163, Max precision: 0.7804, Min precision: 0.719\n",
      "                          Recall: 0.7224 ± 0.0193, Max Recall: 0.7615, Min Recall: 0.6764\n",
      "                          F1: 0.7368 ± 0.017, Max F1: 0.7678, Min F1: 0.6971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## let's get and print test results\n",
    "result_test = get_overall_results(test_results)\n",
    "print(f'Results on test dataset for {len(test_results)} runs:\\n{result_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on validation dataset for 30 runs:\n",
      "tagger_results: Precision: 0.892 ± 0.0111, Max precision: 0.9293, Min precision: 0.8745\n",
      "                Recall: 0.8869 ± 0.0138, Max Recall: 0.9201, Min Recall: 0.8605\n",
      "                F1: 0.8894 ± 0.0114, Max F1: 0.9247, Min F1: 0.8674\n",
      "\n",
      "parser_labeled_results: Precision: 0.6897 ± 0.0195, Max precision: 0.7369, Min precision: 0.6497\n",
      "                        Recall: 0.6562 ± 0.0278, Max Recall: 0.7099, Min Recall: 0.5884\n",
      "                        F1: 0.6724 ± 0.0232, Max F1: 0.7232, Min F1: 0.6175\n",
      "\n",
      "parser_unlabeled_results: Precision: 0.7639 ± 0.0204, Max precision: 0.8114, Min precision: 0.7163\n",
      "                          Recall: 0.7267 ± 0.0294, Max Recall: 0.7821, Min Recall: 0.6488\n",
      "                          F1: 0.7447 ± 0.0244, Max F1: 0.7963, Min F1: 0.6809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## let's get and print validation results\n",
    "result_val = get_overall_results(val_results)\n",
    "print(f'Results on validation dataset for {len(val_results)} runs:\\n{result_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46e1864029c28752d956e1c306007726d1cdf9327c2678741762de7d12d39fb4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('MTrfg_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
