{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In this notebook, we run evaluation on curriculum learning \n",
    "## model using predicted tags. \n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from mtrfg.utils import load_json, get_overall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all dir paths for 30 splits models.\n",
    "\n",
    "split_30_models = glob.glob(\"../saved_models/30_splits_finetune_with_new_softmax_on_old_split/*/model.pth\")\n",
    "split_30_models_dirs = [Path(model_path).parent.absolute() for model_path in split_30_models]\n",
    "split_30_models_dirs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test file path\n",
    "test_file_out = 'test_results_pred_tags.json'\n",
    "test_files = glob.glob('/data/Multitask_RFG/30-splits_old/split_*/test.conllu')\n",
    "test_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### let's change the model dir\n",
    "\n",
    "# split_30_models_dirs = ['../saved_models/Silver_data_pretraining/Recipe1_Silver_data_with_different_softmax_and_gt_tags_bert-base-uncased_2022-10-21--14:50:40'] * len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "## let's evaluate! \n",
    "for i, (dir_path, test_file) in enumerate(zip(split_30_models_dirs, test_files)):\n",
    "    test_file_out = f'old_split_pred_tags.json'\n",
    "    cmd = f'CUDA_VISIBLE_DEVICES=1 python3 ../tools/evaluate.py --opts --dir_name \"{dir_path}\" --test_file \"{test_file}\" --batch_size \"16\" --save_file_name \"{test_file_out}\" --use_pred_tags \"True\"'\n",
    "    if os.system(cmd) != 0:\n",
    "        print(\"Evaluation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## path to test and validation results for all the splits\n",
    "test_jsons = [os.path.join(split_model_dir, f'old_split_pred_tags.json') for i, split_model_dir in enumerate(split_30_models_dirs)]\n",
    "test_results = [load_json(test_file) for test_file in test_jsons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on test dataset for 30 runs:\n",
      "tagger_results: Precision: 0.9071 ± 0.0106, Max precision: 0.924, Min precision: 0.8881\n",
      "                Recall: 0.9098 ± 0.0119, Max Recall: 0.9309, Min Recall: 0.888\n",
      "                F1: 0.9084 ± 0.0105, Max F1: 0.926, Min F1: 0.8893\n",
      "\n",
      "parser_labeled_results: Precision: 0.7643 ± 0.0191, Max precision: 0.7964, Min precision: 0.7197\n",
      "                        Recall: 0.749 ± 0.0195, Max Recall: 0.7824, Min Recall: 0.709\n",
      "                        F1: 0.7565 ± 0.0188, Max F1: 0.7849, Min F1: 0.7152\n",
      "\n",
      "parser_unlabeled_results: Precision: 0.8256 ± 0.0173, Max precision: 0.8623, Min precision: 0.7891\n",
      "                          Recall: 0.809 ± 0.0179, Max Recall: 0.8383, Min Recall: 0.7709\n",
      "                          F1: 0.8172 ± 0.017, Max F1: 0.8501, Min F1: 0.781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_test_pred_tags = get_overall_results(test_results)\n",
    "print(f'Results on test dataset for {len(test_results)} runs:\\n{results_test_pred_tags}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## validation file path\n",
    "val_file_out = 'dev_results_pred_tags.json'\n",
    "val_files = glob.glob('/data/Multitask_RFG/30-splits_2022-09-06--16:49:27/split_*/Parser/dev.conllu')\n",
    "val_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's evaluate! \n",
    "for dir_path, val_file in zip(split_30_models_dirs, val_files):\n",
    "    cmd = f'CUDA_VISIBLE_DEVICE=1 python3 ../tools/evaluate.py --opts --dir_name \"{dir_path}\" --test_file \"{val_file}\" --batch_size \"16\" --save_file_name \"{val_file_out}\" --use_pred_tags \"True\"'\n",
    "    if os.system(cmd) != 0:\n",
    "        print(\"Evaluation failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## path to test and validation results for all the splits\n",
    "val_jsons = [os.path.join(split_model_dir, val_file_out) for split_model_dir in split_30_models_dirs]\n",
    "dev_results = [load_json(val_file) for val_file in val_jsons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_val_pred_tags = get_overall_results(dev_results)\n",
    "print(f'Results on validation dataset for {len(dev_results)} runs:\\n{results_val_pred_tags}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46e1864029c28752d956e1c306007726d1cdf9327c2678741762de7d12d39fb4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('MTrfg_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
