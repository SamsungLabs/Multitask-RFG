{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This notebook is to build a curriculum learning dataset.\n",
    "    Idea:\n",
    "        We have a silver dataset with recipes with variable lengths. Based on our assumption that smaller recipes are easier to train, \n",
    "        we build multiple chunks of silver dataset and progressively introduce them during training. We start with smallest recipes\n",
    "        and slowly introduce longer recipes to ease learning\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "## imports\n",
    "from mtrfg.utils import read_conllu_dataset_allennlp, write_text, make_dir, get_allennlp_in_conllu_format\n",
    "import os\n",
    "from math import floor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's get the relevant paths\n",
    "train_file = '/data/Ahmad/Silver/Parser/train.conllu'\n",
    "test_file = '/data/Ahmad/Silver/Parser/test.conllu'\n",
    "dev_file = '/data/Ahmad/Silver/Parser/dev.conllu'\n",
    "\n",
    "## path to curriculum learning dataset (where data will be stored)\n",
    "path_to_curriculum_data = '/data/Multitask_RFG/curriculum_learning_data/'\n",
    "make_dir(path_to_curriculum_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's load the data\n",
    "train_data = read_conllu_dataset_allennlp(train_file)\n",
    "dev_data = read_conllu_dataset_allennlp(dev_file)\n",
    "test_data = read_conllu_dataset_allennlp(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this will contain certain helper functions that we will need laters\n",
    "\n",
    "def get_curriculum_learning_dataset(input_data, data_splits = 10):\n",
    "    \"\"\"\n",
    "        input_data: List of allennlp Instances\n",
    "        data_splits: How many curriculum learning data splits should be. (If there are 100 input data points, and data_split is 20 then we have 5 distinct dataset from shorter lengths to longer lengths)\n",
    "\n",
    "        Output: List of datasets, each list element is 1 dataset. List is in sorted order, starting elements will have smallest recipes and end elements will have largest recipes\n",
    "\n",
    "    \"\"\"\n",
    "    input_data_dict = {f'recipe_{i}': get_allennlp_in_conllu_format(data_point) for i, data_point in enumerate(input_data)}\n",
    "    input_data_sorted_keys = dict(sorted( {f'recipe_{i}': len(data_point['words'].tokens) for i, data_point in enumerate(input_data)}.items(), key=lambda item: item[1])) ## sort input data by length of the recipes, smallest will come on the top\n",
    "    input_data_sorted_keys = [key for key in input_data_sorted_keys.keys()]\n",
    "\n",
    "    output_datasets = []\n",
    "    dataset_size = floor(len(input_data_sorted_keys) / data_splits)\n",
    "\n",
    "    ## let's get the datasets\n",
    "    for i in range(0, len(input_data_sorted_keys), dataset_size):\n",
    "        keys = input_data_sorted_keys[i : i + dataset_size] ## keys for this batch of dataset\n",
    "        output_datasets.append('\\n\\n'.join([input_data_dict[key] for key in keys]))\n",
    "\n",
    "    return output_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "## define datasplits\n",
    "data_splits = 10\n",
    "\n",
    "## let's build the datasets and save them\n",
    "train_datasets = get_curriculum_learning_dataset(train_data, data_splits = data_splits)\n",
    "dev_datasets = get_curriculum_learning_dataset(dev_data, data_splits = data_splits)\n",
    "test_datasets = get_curriculum_learning_dataset(test_data, data_splits = data_splits)\n",
    "\n",
    "print(len(train_datasets), len(dev_datasets), len(test_datasets))\n",
    "\n",
    "## let's save the datasets!\n",
    "for i in range(data_splits):\n",
    "    train_path, test_path, dev_path = os.path.join(path_to_curriculum_data, f'train_{i}.conllu'), os.path.join(path_to_curriculum_data, f'test_{i}.conllu'), os.path.join(path_to_curriculum_data, f'dev_{i}.conllu')\n",
    "    write_text(train_path, train_datasets[i])\n",
    "    write_text(dev_path, dev_datasets[i])\n",
    "    write_text(test_path, test_datasets[i])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46e1864029c28752d956e1c306007726d1cdf9327c2678741762de7d12d39fb4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('MTrfg_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
