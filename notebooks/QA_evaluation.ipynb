{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtrfg.utils.data_utils import read_conllu_dataset_allennlp\n",
    "import string\n",
    "from pprint import pprint\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## file path where the results are stored as CoNLLU file\n",
    "output_file = f'/user/d.bhatt/Multitask-RFG/saved_models/QA_as_FGP/_deepset-bert-base-uncased-squad2_2023-01-13--13:23:24/test_out.conllu'\n",
    "input_file = f'/data/Multitask_RFG/squad_conllu_dataset/test.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading the output\n",
    "pred_data = read_conllu_dataset_allennlp(output_file)\n",
    "gt_data = read_conllu_dataset_allennlp(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's iterate through data and get answers\n",
    "def get_question_context_answer(datapoint_pred, datapoint_gt):\n",
    "    def remove_space_punctuations(input_string):\n",
    "        \"\"\"\n",
    "            This function is to remove the space that appears before \n",
    "            punctuations due to tokenization, and here we remove it. \n",
    "        \"\"\"\n",
    "        output_string = input_string\n",
    "        for punc in string.punctuation:\n",
    "            output_string = output_string.replace(f' {punc}', f'{punc}')\n",
    "            output_string = output_string.replace(f'{punc} ', f'{punc}')\n",
    "\n",
    "        return output_string\n",
    "\n",
    "\n",
    "    def get_question(datapoint):\n",
    "        \"\"\"\n",
    "            Extract question from datapoint received\n",
    "        \"\"\"\n",
    "        words = [word.text for word in datapoint['words'].tokens]\n",
    "        sep_index = words.index('[SEP]')\n",
    "        question = ' '.join(words[:sep_index])\n",
    "        question = remove_space_punctuations(question)\n",
    "        return question\n",
    "    \n",
    "    def get_context(datapoint):\n",
    "        \"\"\"\n",
    "            Get answer from datapoint received\n",
    "        \"\"\"\n",
    "        words = [word.text for word in datapoint['words'].tokens]\n",
    "        sep_index = words.index('[SEP]') + 1\n",
    "        context = ' '.join(words[sep_index:])\n",
    "        context = remove_space_punctuations(context)\n",
    "        return context\n",
    "\n",
    "    def get_answer(datapoint):\n",
    "        \"\"\"\n",
    "            Extract answer based on tags. \n",
    "            We iterate through tags, and try to find B-ANS, I-ANS followed by it are part of the \n",
    "            answer too\n",
    "        \"\"\"\n",
    "        answers = []\n",
    "        b_ans_found = False\n",
    "        for i, tag in enumerate(datapoint['pos_tags'].labels):\n",
    "            if tag == 'B-ANS':\n",
    "                b_ans_found = True\n",
    "                answers.append(datapoint['words'].tokens[i].text)\n",
    "            elif tag == 'O':\n",
    "                b_ans_found = False\n",
    "            elif tag == 'I-ANS' and b_ans_found:\n",
    "                answers[-1] = answers[-1] + ' ' + datapoint['words'].tokens[i].text\n",
    "\n",
    "        return [remove_space_punctuations(answer) for answer in answers]\n",
    "        \n",
    "\n",
    "    answer_dict = {}\n",
    "    answer_dict['question'] = get_question(datapoint_gt)\n",
    "    answer_dict['gt_ans'] = get_answer(datapoint_gt)\n",
    "    answer_dict['pred_ans'] = get_answer(datapoint_pred)\n",
    "    answer_dict['context'] = get_context(datapoint_gt)\n",
    "\n",
    "    return answer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_answers = {}\n",
    "\n",
    "for i, (pred_data_point, gt_data_point) in enumerate(zip(pred_data, gt_data)):\n",
    "    \"\"\"\n",
    "        Here, we iterate through gt and prediction data, and build a \n",
    "        dictionary with GT and Predicted answers! \n",
    "    \"\"\"\n",
    "    all_answers[f'{i}'.zfill(5)] = get_question_context_answer(pred_data_point, gt_data_point)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(all_answers)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46e1864029c28752d956e1c306007726d1cdf9327c2678741762de7d12d39fb4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('MTrfg_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
