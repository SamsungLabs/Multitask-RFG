"""
    train the model using curriculum learning.
    The dataset provided using --curriculum_data_dir is generated by notebooks/build_curriculum_learning_dataset.ipynb
    How to run: 
        This script takes a few arguments. 
        1. 'dir_path' --> Path where all curriculum learning models are to be stored! 
        2. 'curriculum_data_dir' --> Directory where curriculum learning dataset is located!
        3. 'data_splits' --> Number of data splits where model is trained! 
        4. 'epoch_per_training' --> How many epochs should each model be trained on. 

    CUDA_VISIBLE_DEVICES=1 python3 tools/train_curriculum.py --opts --dir_path "./saved_models/curriculum_learning_models/training_1" --data_splits "10" --epoch_per_training "10" --curriculum_data_dir "/data/Multitask_RFG/curriculum_learning_data" --data_exposure_strategy "bin_by_bin"
"""

import os
from mtrfg.utils import (save_json, 
                        get_curriculum_learning_data_paths,
                        get_args, 
                        get_label_index_mapping, 
                        make_dir,
                        save_python_command
                        )

import sys, glob


## get the arguments to modify the config
args = get_args()  

## here args has some extra arguments to train the model with curriculum learning setup
dir_path = args['dir_path'] ## directory inside which all the different models shall be stored. 
curriculum_data_dir = args['curriculum_data_dir'] ## this is the directory where we expect curriculum learning dataset to be! 
data_splits = args['data_splits'] if 'data_splits' in args else 10
epoch_per_training = args['epoch_per_training'] if 'epoch_per_training' in args else 10 ## how long should I train the model on 1 dataset split
data_exposure_strategy = args['data_exposure_strategy'] if 'data_exposure_strategy' in args else 'bin_by_bin' ## this could be from ['cumulation', 'bin_by_bin']
if data_exposure_strategy not in ['cumulation', 'bin_by_bin']:
    print(f'Dataset strategy {data_exposure_strategy} is undefined. Going with "bin_by_bin" strategy instead.')
    data_exposure_strategy = 'bin_by_bin'

## let's build the label file and store it so all trainings use same label-ID pairs
train_path, _, _ = get_curriculum_learning_data_paths(curriculum_data_dir, until_dataset = data_splits, data_strategy='cumulation') ## here we change data exposure strategy as here we want to get labels map
label_index_map = get_label_index_mapping(train_path)
label_index_map_path = os.path.join(dir_path, 'label_index_map.json')
make_dir(dir_path)
save_json(label_index_map, label_index_map_path)
cmd_file = os.path.join(dir_path, 'file_command.txt')
save_python_command(cmd_file, sys.argv)

## start with model, in the start we don't have any model, so we start with None
model_start_path = None

## let's start training! 
for i in range(data_splits):
    train_path, _, _ = get_curriculum_learning_data_paths(curriculum_data_dir, until_dataset = i + 1, data_strategy=data_exposure_strategy)
    test_path, dev_path = "/data/Ahmad/Silver/Parser/test.conllu", "/data/Ahmad/Silver/Parser/dev.conllu" ## for now, test and validation datasets are hardcoded! 
    model_dir = os.path.join(dir_path, f'model_{i}')

    ## let's run the training
    train_cmd = f'python3 tools/train.py --opts --train_file "{train_path}" --val_file "{dev_path}" --test_file "{test_path}" --epochs "{epoch_per_training}" --save_dir "{model_dir}" --model_name "bert-base-uncased" --seed "42" --use_pred_tags "True" --model_start_path "{model_start_path}" --labels_json_path "{label_index_map_path}" --freeze_until_epoch "{epoch_per_training+1}" --batch_size "16" ' 
    if os.system(train_cmd) != 0:
        print("Curriculum learning traing failed! Please relaunch it again. Removing curriculum learning directory.")
        os.system(f'rm -r {dir_path}')
        sys.exit(0)
    else:
        model_start_path = glob.glob(f'{model_dir}*/*.pth')[0] ## this is the starting point for training the next model!